"""–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ç–µ—Å—Ç—ã –¥–ª—è OllamaProvider —Å —Ä–µ–∞–ª—å–Ω—ã–º Mistral."""

import pytest
from ollama import ResponseError

from avatar.llm.ollama_provider import OllamaProvider
from avatar.schemas.llm_types import Message


# –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ Ollama –ø–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º —Ç–µ—Å—Ç–æ–≤
@pytest.fixture(scope="function")
async def ollama_provider():
    """Fixture –¥–ª—è —Ä–µ–∞–ª—å–Ω–æ–≥–æ OllamaProvider."""
    provider = OllamaProvider(
        model="mistral:7b-instruct-q4_K_M",
        base_url="http://localhost:11434",
    )
    
    # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, —á—Ç–æ Ollama –¥–æ—Å—Ç—É–ø–µ–Ω
    try:
        is_healthy = await provider.healthcheck()
        if not is_healthy:
            pytest.skip("Ollama server is not available or model not found")
    except Exception:
        pytest.skip("Cannot connect to Ollama server")
    
    return provider


@pytest.mark.integration
@pytest.mark.asyncio
async def test_ollama_healthcheck(ollama_provider):
    """–¢–µ—Å—Ç: –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ Ollama –∏ –º–æ–¥–µ–ª–∏ Mistral."""
    is_healthy = await ollama_provider.healthcheck()
    
    assert is_healthy is True, "Ollama healthcheck failed"


@pytest.mark.integration
@pytest.mark.asyncio
async def test_ollama_generate_simple(ollama_provider):
    """–¢–µ—Å—Ç: –ø—Ä–æ—Å—Ç–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞."""
    messages = [
        Message(role="user", content="–ü—Ä–∏–≤–µ—Ç! –ö–∞–∫ –¥–µ–ª–∞?")
    ]
    
    response = await ollama_provider.generate(messages, temperature=0.7, max_tokens=100)
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∏
    assert response.text is not None
    assert len(response.text) > 0, "Generated text is empty"
    assert response.tokens_count > 0, "Token count should be positive"
    assert response.generation_time >= 0.0, "Generation time should be non-negative"
    
    print(f"\n‚úÖ Generated: {response.text}")
    print(f"‚è±Ô∏è  Time: {response.generation_time:.2f}s")
    print(f"üî¢ Tokens: {response.tokens_count}")


@pytest.mark.integration
@pytest.mark.asyncio
async def test_ollama_generate_with_history(ollama_provider):
    """–¢–µ—Å—Ç: –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Å –∏—Å—Ç–æ—Ä–∏–µ–π –¥–∏–∞–ª–æ–≥–∞."""
    messages = [
        Message(role="user", content="–ú–µ–Ω—è –∑–æ–≤—É—Ç –ê–ª–µ–∫—Å."),
        Message(role="assistant", content="–ü—Ä–∏—è—Ç–Ω–æ –ø–æ–∑–Ω–∞–∫–æ–º–∏—Ç—å—Å—è, –ê–ª–µ–∫—Å!"),
        Message(role="user", content="–ö–∞–∫ –º–µ–Ω—è –∑–æ–≤—É—Ç?"),
    ]
    
    response = await ollama_provider.generate(messages, temperature=0.5, max_tokens=50)
    
    assert response.text is not None
    assert len(response.text) > 0
    # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, —á—Ç–æ –º–æ–¥–µ–ª—å –ø–æ–º–Ω–∏—Ç –∏–º—è
    assert "–∞–ª–µ–∫—Å" in response.text.lower(), "Model should remember the name from context"
    
    print(f"\n‚úÖ Context-aware response: {response.text}")


@pytest.mark.integration
@pytest.mark.asyncio
async def test_ollama_generate_stream(ollama_provider):
    """–¢–µ—Å—Ç: –ø–æ—Ç–æ–∫–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞."""
    messages = [
        Message(role="user", content="–†–∞—Å—Å–∫–∞–∂–∏ –∫–æ—Ä–æ—Ç–∫—É—é –∏—Å—Ç–æ—Ä–∏—é –ø—Ä–æ –∫–æ—Ç–∞.")
    ]
    
    chunks = []
    token_count = 0
    
    async for chunk in ollama_provider.generate_stream(messages, temperature=0.8, max_tokens=150):
        chunks.append(chunk)
        token_count += 1
        print(chunk, end="", flush=True)  # –ü–µ—á–∞—Ç–∞—Ç—å —Ç–æ–∫–µ–Ω—ã –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
    
    full_text = "".join(chunks)
    
    assert len(chunks) > 0, "No chunks received from stream"
    assert len(full_text) > 0, "Generated text is empty"
    assert token_count > 0, "Token count should be positive"
    
    print(f"\n\n‚úÖ Streamed {token_count} tokens")
    print(f"üìù Full text: {full_text[:100]}...")


@pytest.mark.integration
@pytest.mark.asyncio
async def test_ollama_temperature_variation(ollama_provider):
    """–¢–µ—Å—Ç: –≤–ª–∏—è–Ω–∏–µ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã –Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é."""
    messages = [
        Message(role="user", content="–°–∫–∞–∂–∏ –æ–¥–Ω–æ —Å–ª–æ–≤–æ: –ø—Ä–∏–≤–µ—Ç –∏–ª–∏ –∑–¥—Ä–∞–≤—Å—Ç–≤—É–π")
    ]
    
    # –ù–∏–∑–∫–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ (–¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π)
    response_low = await ollama_provider.generate(messages, temperature=0.1, max_tokens=10)
    
    # –í—ã—Å–æ–∫–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ (–∫—Ä–µ–∞—Ç–∏–≤–Ω—ã–π)
    response_high = await ollama_provider.generate(messages, temperature=1.5, max_tokens=10)
    
    assert response_low.text is not None
    assert response_high.text is not None
    
    print(f"\nüîµ Low temp (0.1): {response_low.text}")
    print(f"üî¥ High temp (1.5): {response_high.text}")


@pytest.mark.integration
@pytest.mark.asyncio
async def test_ollama_max_tokens_limit(ollama_provider):
    """–¢–µ—Å—Ç: –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ max_tokens."""
    messages = [
        Message(role="user", content="–†–∞—Å—Å–∫–∞–∂–∏ –¥–ª–∏–Ω–Ω—É—é –∏—Å—Ç–æ—Ä–∏—é –æ –ø—É—Ç–µ—à–µ—Å—Ç–≤–∏–∏.")
    ]
    
    response = await ollama_provider.generate(messages, temperature=0.7, max_tokens=20)
    
    # –¢–æ–∫–µ–Ω–æ–≤ –Ω–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –±–æ–ª—å—à–µ, —á–µ–º max_tokens
    assert response.tokens_count <= 20, f"Token count {response.tokens_count} exceeds max_tokens=20"
    
    print(f"\n‚úÖ Generated {response.tokens_count} tokens (max: 20)")
    print(f"üìù Text: {response.text}")


@pytest.mark.integration
@pytest.mark.asyncio
async def test_ollama_russian_language(ollama_provider):
    """–¢–µ—Å—Ç: –ø–æ–¥–¥–µ—Ä–∂–∫–∞ —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞."""
    messages = [
        Message(role="user", content="–ü–µ—Ä–µ–≤–µ–¥–∏ –Ω–∞ —Ä—É—Å—Å–∫–∏–π: Hello, how are you?")
    ]
    
    response = await ollama_provider.generate(messages, temperature=0.5, max_tokens=50)
    
    # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, —á—Ç–æ –≤ –æ—Ç–≤–µ—Ç–µ –µ—Å—Ç—å –∫–∏—Ä–∏–ª–ª–∏—Ü–∞
    has_cyrillic = any('\u0400' <= char <= '\u04FF' for char in response.text)
    assert has_cyrillic, "Response should contain Cyrillic characters"
    
    print(f"\n‚úÖ Russian response: {response.text}")


@pytest.mark.integration
@pytest.mark.asyncio
async def test_ollama_error_handling_empty_messages(ollama_provider):
    """–¢–µ—Å—Ç: –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–∫–∏ –ø—Ä–∏ –ø—É—Å—Ç—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏—è—Ö."""
    with pytest.raises(ValueError, match="messages list cannot be empty"):
        await ollama_provider.generate([], temperature=0.7, max_tokens=100)


@pytest.mark.integration
@pytest.mark.asyncio
async def test_ollama_error_handling_invalid_temperature(ollama_provider):
    """–¢–µ—Å—Ç: –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–∫–∏ –ø—Ä–∏ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–µ."""
    messages = [Message(role="user", content="Test")]
    
    with pytest.raises(ValueError, match="temperature must be between"):
        await ollama_provider.generate(messages, temperature=3.0, max_tokens=100)


@pytest.mark.integration
@pytest.mark.asyncio
async def test_ollama_error_handling_invalid_max_tokens(ollama_provider):
    """–¢–µ—Å—Ç: –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–∫–∏ –ø—Ä–∏ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–º max_tokens."""
    messages = [Message(role="user", content="Test")]
    
    with pytest.raises(ValueError, match="max_tokens must be between"):
        await ollama_provider.generate(messages, temperature=0.7, max_tokens=5000)

